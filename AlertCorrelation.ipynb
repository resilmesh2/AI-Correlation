{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tengfei/anaconda3/envs/pytorch-1.12.1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import Counter\n",
    "sys.path.append('..')\n",
    "\n",
    "# DeepCASE Imports\n",
    "from deepcase.preprocessing   import Preprocessor\n",
    "from deepcase.context_builder import ContextBuilder\n",
    "from deepcase.interpreter     import Interpreter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "DEBUG = True\n",
    "context_length = 50\n",
    "\n",
    "if DEBUG:\n",
    "    config = {\n",
    "    'learning_rates': [0.001],\n",
    "    'eps':            [0.2],        # Epsilon value to use for DBSCAN clustering, in paper this was 0.1 \n",
    "    'threshold':      [0.2]\n",
    "    }\n",
    "    Epochs = 2\n",
    "else:\n",
    "    config = {\n",
    "        'learning_rates': [0.001,0.0001,0.00001],\n",
    "        'eps':            [0.3,0.2,0.15,0.1],        # Epsilon value to use for DBSCAN clustering, in paper this was 0.1 \n",
    "        'threshold':      [0.2,0.15,0.1,0.05,0.01]         # Confidence threshold used for determining if attention from the ContextBuilder can be used, in paper this was 0.2\n",
    "        }\n",
    "    Epochs = 50\n",
    "\n",
    "########################################################################\n",
    "#                             Loading data                             #\n",
    "########################################################################\n",
    "\n",
    "# Create preprocessor\n",
    "preprocessor = Preprocessor(\n",
    "    length  = context_length,    # 10 events in context\n",
    "    timeout = 86400, # Ignore events older than 1 day (60*60*24 = 86400 seconds)\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# prepare data-drop duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for find origin index from data file, through 'indexIntraining' in Interpreter.predict()\n",
    "'''\n",
    "# Load data from file\n",
    "fox_alerts_ori = '../Datasets/AIT-ADS/alerts_csv_labels/fox_alerts.txt'\n",
    "harrison_alerts_ori = '../Datasets/AIT-ADS/alerts_csv_labels/harrison_alerts.txt'\n",
    "russellmitchell_alerts_ori = '../Datasets/AIT-ADS/alerts_csv_labels/russellmitchell_alerts.txt'\n",
    "santos_alerts_ori = '../Datasets/AIT-ADS/alerts_csv_labels/santos_alerts.txt'\n",
    "shaw_alerts_ori = '../Datasets/AIT-ADS/alerts_csv_labels/shaw_alerts.txt'\n",
    "wardbeck_alerts_ori = '../Datasets/AIT-ADS/alerts_csv_labels/wardbeck_alerts.txt'\n",
    "wheeler_alerts_ori = '../Datasets/AIT-ADS/alerts_csv_labels/wheeler_alerts.txt'\n",
    "wilson_alerts_ori = '../Datasets/AIT-ADS/alerts_csv_labels/wilson_alerts.txt'\n",
    "\n",
    "# find the index of duplicated records.\n",
    "filelist=[fox_alerts_ori,harrison_alerts_ori,russellmitchell_alerts_ori,santos_alerts_ori,shaw_alerts_ori,wardbeck_alerts_ori,wheeler_alerts_ori,wilson_alerts_ori]\n",
    "for i in range(len(filelist)):\n",
    "    data_temp = pd.read_csv(filelist[i],sep=',')\n",
    "    print(filelist[i])\n",
    "    print(len(data_temp))\n",
    "    dup_bool=data_temp.duplicated(subset=['time','ip','short']) \n",
    "    dup_indexs=np.where(dup_bool==False)[0]    \n",
    "    print(len(dup_indexs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicated records and store in new files.\n",
    "filelist=[fox_alerts,harrison_alerts,russellmitchell_alerts,santos_alerts,shaw_alerts,wardbeck_alerts,wheeler_alerts,wilson_alerts]\n",
    "for i in range(len(filelist)):\n",
    "    data_temp = pd.read_csv(filelist[i],sep=',')\n",
    "    print(filelist[i])    \n",
    "    data_temp = data_temp.drop_duplicates(subset=['time','ip','short']) \n",
    "    # Save DataFrame to a text file\n",
    "    new_name=filelist[i].split('.txt')[0]+'_uniq'+'.txt'\n",
    "    data_temp.to_csv(new_name, index=False) \n",
    "    print(\"DataFrame saved to \"+new_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepCASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from file\n",
    "fox_alerts = '../Datasets/AIT-ADS/alerts_csv_labels/fox_alerts_uniq.txt'\n",
    "harrison_alerts = '../Datasets/AIT-ADS/alerts_csv_labels/harrison_alerts_uniq.txt'\n",
    "russellmitchell_alerts = '../Datasets/AIT-ADS/alerts_csv_labels/russellmitchell_alerts_uniq.txt'\n",
    "santos_alerts = '../Datasets/AIT-ADS/alerts_csv_labels/santos_alerts_uniq.txt'\n",
    "shaw_alerts = '../Datasets/AIT-ADS/alerts_csv_labels/shaw_alerts_uniq.txt'\n",
    "wardbeck_alerts = '../Datasets/AIT-ADS/alerts_csv_labels/wardbeck_alerts_uniq.txt'\n",
    "wheeler_alerts = '../Datasets/AIT-ADS/alerts_csv_labels/wheeler_alerts_uniq.txt'\n",
    "wilson_alerts = '../Datasets/AIT-ADS/alerts_csv_labels/wilson_alerts_uniq.txt'\n",
    "\n",
    "data =preprocessor.read_csv_files(fox_alerts,harrison_alerts,russellmitchell_alerts,santos_alerts,shaw_alerts,wardbeck_alerts,wheeler_alerts,wilson_alerts)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data = data.rename(columns = {\"time\":\"timestamp\",\"ip\":\"machine\",\"short\":\"event\",\"time_label\":\"label\"})\n",
    "context, events,  mapping, mapping_label, labels=preprocessor.sequence(data,labels=None,verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of samples: 380588\n",
      "the number of false positive: 357548\n",
      "the number of analomal samples: 23040 \n"
     ]
    }
   ],
   "source": [
    "print(f'the total number of samples: {len(labels)}')\n",
    "print(f'the number of false positive: {sum(labels==0)}')\n",
    "print(f'the number of analomal samples: {sum(labels==1)} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case no labels are provided, set labels to -1\n",
    "# IMPORTANT: If no labels are provided, make sure to manually set the labels\n",
    "# before calling the interpreter.score_clusters method. Otherwise, this will\n",
    "# raise an exception, because scores == NO_SCORE cannot be computed.\n",
    "if labels is None:\n",
    "    labels = np.full(events.shape[0], -1, dtype=int)    \n",
    "    \n",
    "''' using pandas.dataframe to save index for all data need to split through using train_test_split() ; \n",
    "context, labels, and events need to split at the same time for corresponse.(TF)'''\n",
    "frames=[data,pd.DataFrame(context.detach().numpy()) ]  # \n",
    "X = pd.concat(frames, axis=1)  \n",
    "y = pd.Series(labels)\n",
    "X_train, X_test, labels_train_binary,labels_test_binary = train_test_split(X,y,test_size=0.4,random_state=35)\n",
    "\n",
    "context_train=torch.tensor( X_train.loc[:,0:(context_length-1)].to_numpy() )\n",
    "events_train=torch.tensor( X_train['event'].to_numpy()  )\n",
    "context_test=torch.tensor( X_test.loc[:,0:(context_length-1)].to_numpy()   )\n",
    "events_test=torch.tensor( X_test['event'].to_numpy()   )\n",
    "\n",
    "labels_train_binary = torch.tensor(labels_train_binary.to_numpy() )\n",
    "labels_test_binary = torch.tensor(labels_test_binary.to_numpy() )\n",
    "    \n",
    "# Cast to cuda if available\n",
    "if torch.cuda.is_available():\n",
    "    events_train  = events_train.to('cuda')\n",
    "    events_test  = events_test.to('cuda')\n",
    "    context_train = context_train.to('cuda')\n",
    "    context_test = context_test.to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 1/2 loss=0.015762]: 100%|████████████████████████████████████████████████████| 1784/1784 [00:42<00:00, 42.26it/s]\n",
      "[Epoch 2/2 loss=0.013399]: 100%|████████████████████████████████████████████████████| 1784/1784 [00:40<00:00, 44.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ContextBuilder(\n",
       "  (embedding): Embedding(76, 128)\n",
       "  (embedding_one_hot): EmbeddingOneHot()\n",
       "  (encoder): Encoder(\n",
       "    (embedding): EmbeddingOneHot()\n",
       "    (recurrent): GRU(76, 128, batch_first=True)\n",
       "  )\n",
       "  (decoder_attention): DecoderAttention(\n",
       "    (embedding): Embedding(76, 128)\n",
       "    (recurrent): GRU(128, 128, batch_first=True)\n",
       "    (attn): Linear(in_features=128, out_features=50, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder_event): DecoderEventTransformer(\n",
       "    (em): Sequential(\n",
       "      (0): Linear(in_features=76, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (transformer_layer): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (out): Linear(in_features=128, out_features=76, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################################################\n",
    "#                         Using ContextBuilder                         #\n",
    "########################################################################\n",
    "\n",
    "vocab_size = len(np.unique(list(mapping)))\n",
    "\n",
    "context_builder = ContextBuilder(\n",
    "    input_size    = vocab_size,   # Number of input features to expect\n",
    "    output_size   = vocab_size,   # Same as input size\n",
    "    hidden_size   = 128,   # Number of nodes in hidden layer, in paper we set this to 128\n",
    "    max_length    = context_length,    # Length of the context, should be same as context in Preprocessor\n",
    ")\n",
    "\n",
    "# Cast to cuda if available\n",
    "if torch.cuda.is_available():\n",
    "    context_builder = context_builder.to('cuda')\n",
    "\n",
    "# Train the ContextBuilder\n",
    "context_builder.fit(\n",
    "    X             = context_train,               # Context to train with\n",
    "    y             = events_train.reshape(-1, 1), # Events to train with, note that these should be of shape=(n_events, 1)\n",
    "    labels        = labels_train_binary,\n",
    "    epochs        = Epochs,                          # Number of epochs to train with\n",
    "    batch_size    = 128,                         # Number of samples in each training batch, in paper this was 128\n",
    "    learning_rate = 0.001,                        # Learning rate to train with, in paper this was 0.01\n",
    "    verbose       = True,                        # If True, prints progress\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing query: 100%|███████████████████████████████████████████████████████████| 18900/18900 [05:36<00:00, 56.19it/s]\n",
      "Clustering: 100%|███████████████████████████████████████████████████████████████████████| 20/20 [02:23<00:00,  7.18s/it]\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "#                          Using Interpreter                           #\n",
    "########################################################################\n",
    "\n",
    "interpreter = Interpreter(\n",
    "    context_builder = context_builder, # ContextBuilder used to fit data\n",
    "    features        = 100,             # Number of input features to expect, should be same as ContextBuilder\n",
    "    eps             = 0.1,             # Epsilon value to use for DBSCAN clustering, in paper this was 0.1  # try 0.2 Jia\n",
    "    min_samples     = 5,               # Minimum number of samples to use for DBSCAN clustering, in paper this was 5\n",
    "    threshold       = 0.2,             # Confidence threshold used for determining if attention from the ContextBuilder can be used, in paper this was 0.2\n",
    ")\n",
    "\n",
    "# Cluster samples with the interpreter\n",
    "clusters = interpreter.cluster(\n",
    "    X          = context_train,               # Context to train with\n",
    "    y          = events_train.reshape(-1, 1), # Events to train with, note that these should be of shape=(n_events, 1)\n",
    "    iterations = 100,                         # Number of iterations to use for attention query, in paper this was 100\n",
    "    batch_size = 1024,                        # Batch size to use for attention query, used to limit CUDA memory usage\n",
    "    verbose    = True,                        # If True, prints progress\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring: 100%|██████████████████████████████████████████████████████████████████████████| 17/17 [00:05<00:00,  2.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<deepcase.interpreter.interpreter.Interpreter at 0x7f1ca04e9610>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################################################\n",
    "#                             Manual mode                              #\n",
    "########################################################################\n",
    "\n",
    "# Compute scores for each cluster based on individual labels per sequence\n",
    "scores = interpreter.score_clusters(\n",
    "    scores   = labels_train_binary, # Labels used to compute score (either as loaded by Preprocessor, or put your own labels here)\n",
    "    strategy = \"min\",        # Strategy to use for scoring (one of \"max\", \"min\", \"avg\")\n",
    "    NO_SCORE = -1,           # Any sequence with this score will be ignored in the strategy.\n",
    "                                # If assigned a cluster, the sequence will inherit the cluster score.\n",
    "                                # If the sequence is not present in a cluster, it will receive a score of NO_SCORE.\n",
    ")\n",
    "\n",
    "# Assign scores to clusters in interpreter\n",
    "# Note that all sequences should be given a score and each sequence in the\n",
    "# same cluster should have the same score.\n",
    "interpreter.score(\n",
    "    scores  = scores, # Scores to assign to sequences\n",
    "    verbose = True,   # If True, prints progress\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "#                        (Semi-)Automatic mode                         #\n",
    "########################################################################\n",
    "\n",
    "#Compute predicted scores\n",
    "prediction, indexIntraining = interpreter.predict(\n",
    "    X          = context_test,               # Context to predict\n",
    "    y          = events_test.reshape(-1, 1), # Events to predict, note that these should be of shape=(n_events, 1)\n",
    "    iterations = 100,                        # Number of iterations to use for attention query, in paper this was 100\n",
    "    batch_size = 1024,                       # Batch size to use for attention query, used to limit CUDA memory usage\n",
    "    verbose    = True,                       # If True, prints progress\n",
    ")\n",
    "\n",
    "#Compute the accuracy\n",
    "mask_p_n = np.where((prediction ==0) | (prediction ==1))[0]\n",
    "result_predicted = prediction[mask_p_n]\n",
    "labels_test_bin = labels_test_binary[mask_p_n]\n",
    "with open(\"output.txt\", \"a\") as f:\n",
    "    print(classification_report(labels_test_bin, result_predicted,digits=4,zero_division=0.0),file=f)\n",
    "    print(f'the total number of samples: {prediction.shape[0]}', file=f)\n",
    "    print(f'the number of predicted  samples: {sum(prediction ==1)+sum(prediction ==0)}',file=f)\n",
    "    print(f'the percentage of predicted samples:{(sum(prediction ==1)+sum(prediction ==0))/prediction.shape[0]}',file=f)\n",
    "    print(f'the number of samples that can not predicted automatically:{prediction.shape[0]-sum(prediction ==1)-sum(prediction ==0)}',file=f)\n",
    "    print(f'the number of predicted positive samples: {sum(prediction ==1)}',file=f)\n",
    "    print(f'the number of predicted negative samples:{sum(prediction ==0)}',file=f)\n",
    "    print(f'the experiment date is:{time.ctime()}',file=f)\n",
    "    print(50 *'*',file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9830    1.0000    0.9914    131256\n",
      "           1     1.0000    0.2543    0.4054      3052\n",
      "\n",
      "    accuracy                         0.9831    134308\n",
      "   macro avg     0.9915    0.6271    0.6984    134308\n",
      "weighted avg     0.9833    0.9831    0.9781    134308\n",
      "\n",
      "the total number of samples: 152236\n",
      "the number of predicted  samples: 134308\n",
      "the percentage of predicted samples:0.8822354764970178\n",
      "the number of samples that can not predicted automatically:17928\n",
      "the number of predicted positive samples: 776\n",
      "the number of predicted false positive samples:133532\n",
      "the experiment date is:Wed Dec  4 16:12:56 2024\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels_test_bin, result_predicted,digits=4,zero_division=0.0))\n",
    "print(f'the total number of samples: {prediction.shape[0]}')\n",
    "print(f'the number of predicted  samples: {sum(prediction ==1)+sum(prediction ==0)}')\n",
    "print(f'the percentage of predicted samples:{(sum(prediction ==1)+sum(prediction ==0))/prediction.shape[0]}')\n",
    "print(f'the number of samples that can not predicted automatically:{prediction.shape[0]-sum(prediction ==1)-sum(prediction ==0)}')\n",
    "print(f'the number of predicted positive samples: {sum(prediction ==1)}')\n",
    "print(f'the number of predicted false positive samples:{sum(prediction ==0)}')\n",
    "print(f'the experiment date is:{time.ctime()}')\n",
    "print(50 *'*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM (Ollama 3.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare the input data to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sort= X_train.sort_values(by='timestamp')\n",
    "\n",
    "def get_example_context(index_in_prediction, context_number):\n",
    "    example_record=X_train.iloc[indexIntraining[index_in_prediction]]\n",
    "    X_train_sort_machine=X_train_sort.loc[(X_train_sort['machine'] == example_record['machine']) & (X_train_sort['timestamp'] <= example_record['timestamp'])]                           \n",
    "    return X_train_sort_machine.iloc[context_number:,:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_context_number=-11  # context( =10) + event = 11\n",
    "\n",
    "# example_normal_predic_index=np.where(prediction==1)[0][0]   # the example of the first attack record\n",
    "example_normal_predic_index=np.where(prediction==0)[0][0]  # the example of the first false positive record\n",
    "\n",
    "predict_event=X_test.iloc[example_normal_predic_index, :7] # the event record to predict\n",
    "    \n",
    "''' transfer the flag of 'event' and 'label' back to words for Ollama to explain'''\n",
    "for k,v in mapping.items():\n",
    "    X_train_sort.loc[X_train_sort['event']==k,'event']=v\n",
    "    if predict_event.loc['event']==k:\n",
    "        predict_event.loc['event']=v\n",
    "    \n",
    "for k,v in mapping_label.items():    \n",
    "    X_train_sort.loc[X_train_sort['label']==k,'label']=v\n",
    "    if predict_event.loc['label']==k:\n",
    "        predict_event.loc['label']=v\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1769042/973161820.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  predict_withcontext=predict_withcontext.append(predict_event)  # pd.DataFrame(.cpu())])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predict_withcontext=get_example_context(example_normal_predic_index,example_context_number)\n",
    "predict_withcontext=predict_withcontext.append(predict_event)  # pd.DataFrame(.cpu())])\n",
    "\n",
    "example_number=5\n",
    "examples_record=[]\n",
    "for ind in range(example_number):\n",
    "    example_record=X_train.iloc[random.randint(0, X_train.shape[0] +example_context_number-1)]\n",
    "    X_train_sort_machine=X_train_sort.loc[(X_train_sort['machine'] == example_record['machine']) & (X_train_sort['timestamp'] <= example_record['timestamp'])]                           \n",
    "    examples_record.append( X_train_sort_machine.iloc[example_context_number:,:7] )  # the last one event should be the target event.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summizeCorpus(df):\n",
    "    # summarize for the events at the same ip address\n",
    "    # return: list results\n",
    "    # Initialize variables\n",
    "    start_time = df.iloc[0]['timestamp']\n",
    "    pre_event  = df.iloc[0]['name']\n",
    "    pre_host   = df.iloc[0]['host']\n",
    "    pre_short  = df.iloc[0]['event']\n",
    "    pre_label  = df.iloc[0]['label']\n",
    "    count = 1\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in range(1, len(df)):\n",
    "        row = df.iloc[i]\n",
    "        if (row['name'] == pre_event) and (row['host'] == pre_host) and (row['event'] == pre_short) and (row['label'] == pre_label):\n",
    "            count += 1\n",
    "        else:\n",
    "            end_time = df.iloc[i-1]['timestamp']\n",
    "            results.append(f\"{start_time}-{end_time}  {pre_event}  {pre_host} {pre_short}  {pre_label}  counts:{count}\")\n",
    "            \n",
    "            # reset start time\n",
    "            start_time = row['timestamp']\n",
    "            pre_event  = row['name']\n",
    "            pre_host   = row['host']\n",
    "            #pre_short  = row['short']\n",
    "            pre_label  = row['label']\n",
    "            count = 1\n",
    "    end_time = df.iloc[-1]['timestamp']\n",
    "    results.append(f\"{start_time}-{end_time}  {pre_event}  {pre_host}  {pre_label}  counts:{count}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def groupby_ip_event(df,abbr1 = 'machine', abbr2='event'):\n",
    "    \n",
    "    grouped_df = df.groupby([abbr1,abbr2])\n",
    "    \n",
    "    return grouped_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group_ip_event = groupby_ip_event(X_train_sort)\n",
    "\n",
    "#item 1: data to train for getting the rules in llm.\n",
    "X_train_sort_IP=pd.concat(subgroup for _, subgroup in data_group_ip_event)  # data for training in llm\n",
    "X_train_sort_IP_sum=summizeCorpus(X_train_sort_IP)\n",
    "\n",
    "''' the train set for llm should be the concluded/processed context, e.g. from contextbuilder. It should not be the event sequence following time.\n",
    "item2: data for examples in prediction in llm.  examples_record  ( not limited to one IP)\n",
    "item3: context data for prediction event in prediction in llm.  predict_withcontext\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var4prompt(df_list, n=4):\n",
    "    \"\"\" Assign value to variables in the prompt\n",
    "        Input: list contains 5 df, each df has consecutive 11 events\n",
    "               n is the length of df_list, the first 4 frames are samples, the 5th is sample to predict\n",
    "        Output: Dict variables, the keys are:\n",
    "                                example_1\n",
    "                                event_1\n",
    "                                label_1\n",
    "                                ...\n",
    "                                example_4\n",
    "                                event_4\n",
    "                                label_4\n",
    "                                \n",
    "                                task_precedings\n",
    "                                task_event\n",
    "                                  \"\"\"\n",
    "    context = {}\n",
    "    for i in range(n):\n",
    "        \n",
    "        df = df_list[i]\n",
    "        array_df = df.to_numpy() # convert df to a array\n",
    "        \n",
    "        context[f\"example_{i+1}\"] = array_df[0:10,0:5]\n",
    "        context[f\"event_{i+1}\"]   = array_df[10][0:5]\n",
    "        context[f\"label_{i+1}\"]   = array_df[10][5]        \n",
    "    \n",
    "    return context    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LLM detection and explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Ollma package, implement using text completion'''\n",
    "from langchain_ollama.llms import OllamaLLM \n",
    "llm = OllamaLLM(base_url=\"http://192.168.200.204:11434\",model=\"llama3.1:8b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pattern_extraction_prompt_rule = \"\"\" You are a cybersecurity analysis expert. I have a dataset of security samples. Each row is a sample containing the following fields:\n",
    "- Time period (start time and end time)\n",
    "- Event type\n",
    "- Host\n",
    "- Class label of this event (such as \"false positive\")\n",
    "- Counts of this event during this period\n",
    "\n",
    "Your task is to analyze and summarize the patterns and dependencies based on the following two aspects:\n",
    "\n",
    "1. **Analyze the following sequence of events. Identify any patterns or recurring behaviors in this chunk that could lead to a potential outcome or alert. \\\n",
    "    Describe the conditions under which certain events lead to specific outcomes.\n",
    "\n",
    "2. **From the following chunk of events, extract potential causal relationships between event types and their outcomes. \\\n",
    "    Summarize the conditions under which an event seems to lead to a specific action or state.\n",
    "\n",
    "\n",
    "\n",
    "### Example Data (time period, event type, host, class label) are as follows: {sum_ip}\n",
    "\n",
    "\n",
    "**Instructions**:\n",
    "- Format the analysis as a **numbered list**, highlighting significant findings in each of the five aspects.\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " prompt_rule = \"\"\" You are a cybersecurity analysis expert. I have a dataset of security samples. Each row is a sample containing the following fields:\n",
    "- Time period (start time and end time)\n",
    "- Event type\n",
    "- Host\n",
    "- Class label of this event (such as \"false positive\")\n",
    "- Counts of this event during this period\n",
    "\n",
    "Your task is to analyze and summarize the patterns and dependencies based on the following five aspects:\n",
    "\n",
    "1. **Event Sequence Dependency**:\n",
    "   - Summarize any notable patterns where certain events tend to follow other events in sequence.\n",
    "   - Identify any common sequences where certain types of event occurs before others.\n",
    "\n",
    "2. **Time Dependency**:\n",
    "   - Summarize patterns in the timing of events. Do certain events occur more frequently during specific times of the day or specific time periods?\n",
    "   - Identify if there are any time-based patterns, such as events that occur during the night or specific hours.\n",
    "\n",
    "3. **Host Dependency**:\n",
    "   - Summarize patterns where specific hosts are more likely to experience certain types of events.\n",
    "   - Identify any hosts that are frequently associated with certain events or attacks.\n",
    "\n",
    "4. **Event Frequency and Volume Dependency**:\n",
    "   - Identify patterns in the frequency and volume of events, especially when certain events occur in high volumes or low frequencies.\n",
    "   - Recognize if large volumes of events in a short period could indicate floods or DDoS attacks, or if low-frequency events suggest stealthy, persistent threats.\n",
    "\n",
    "5. **Event Evolution and Escalation Patterns**:\n",
    "   - Analyze if certain low-priority events often escalate into more serious incidents.\n",
    "   - Look for patterns where an event sequence might start with a benign or low-severity event but escalate to a more severe attack.\n",
    "\n",
    "### Example Data (time period, event type, host, class label) are as follows: {sum_ip}\n",
    "\n",
    "\n",
    "**Instructions**:\n",
    "- Format the analysis as a **numbered list**, highlighting significant findings in each of the five aspects.\n",
    "            \"\"\"\n",
    "        \n",
    "prompt_rule_update = \"\"\" You are a cybersecurity analysis expert. Based on the previous data and summarized rules, I now have additional samples for you to analyze. Each row is a sample containing the following fields:\n",
    "- Time period (start time and end time)\n",
    "- Event type\n",
    "- Host\n",
    "- Class label of this event (such as \"false positive\")\n",
    "- Counts of this event during this period\n",
    "                Here are the previously summarized rules: \n",
    "                {response}\n",
    "                \n",
    "                The summarized rules are analysed from 5 aspects.\n",
    "                \n",
    "                Following are the new data: \n",
    "                {sum_ip}\n",
    "                \n",
    "\n",
    "               ### Instructions:\n",
    "               1. Analyze the samples according to the 5 aspects of the previous summarized rules.\n",
    "               2. Integrate any new patterns or dependencies found from the new data into the corresponding category in the 5 aspects.\n",
    "               3. **Ensure that your analysis follows the same numbered format** as the previous summarized rules.\n",
    "               4. If the new data introduces new patterns, add new rules to the relevant aspect and clearly explain how they relate to the previously summarized rules.\n",
    "               5. **Provide the final updated summary in the form of a numbered list**, with each of the 5 aspects updated accordingly. \"\"\"\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the 0th batch data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1769042/1082800399.py:11: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  rules = llm(prompt_rule_formated)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the 1th batch data\n",
      "Reading the 2th batch data\n",
      "Reading the 3th batch data\n",
      "Reading the 4th batch data\n",
      "Based on the provided data, I will analyze the samples according to the 5 aspects of the previous summarized rules.\n",
      "\n",
      "**1. Event Sequence Dependency:**\n",
      "\n",
      "The new data still shows that DNS-related events (A-Dns-Clc1) are often preceded by false_positive labels, suggesting initial reconnaissance followed by malicious activity. Additionally, A-Dns-Clc1 events are consistently followed by another A-Dns-Clc1 event with a varying number of false_positive events in between.\n",
      "\n",
      "However, I notice that the new data introduces an additional pattern: S-Flw-Apt (false_positive) events seem to be following each other in a consistent sequence. This suggests that the attacker is becoming even more consistent in their false_positive attempts.\n",
      "\n",
      "**Updated Analysis:** The sequence of DNS-related events (A-Dns-Clc1) still suggests initial reconnaissance followed by malicious activity, with an added pattern of repeated A-Dns-Clc1 events and increasingly consistent S-Flw-Apt events.\n",
      "\n",
      "**2. Event Types:**\n",
      "\n",
      "The new data confirms that A-Dns-Clc1 events are still predominantly occurring on the 45th day (with a count of 55), while S-Flw-Apt events are consistently occurring with a count of 75, 75, and 75 on days 238502, 238507, and 238510, respectively.\n",
      "\n",
      "**Updated Analysis:** A-Dns-Clc1 events continue to predominantly occur on the 45th day (with a count of 55), while S-Flw-Apt events remain consistent with counts of 75 on multiple consecutive days.\n",
      "\n",
      "**3. Event Counts:**\n",
      "\n",
      "The new data shows that A-Dns-Clc1 events have varying counts, ranging from 34 to 57. Meanwhile, S-Flw-Apt events consistently occur with a count of 75.\n",
      "\n",
      "**Updated Analysis:** A-Dns-Clc1 events have varying counts (ranging from 34 to 57), while S-Flw-Apt events continue to consistently occur with a count of 75.\n",
      "\n",
      "**4. Event Days:**\n",
      "\n",
      "The new data shows that A-Dns-Clc1 events predominantly occur on days ranging from 259916 to 260275, with the majority occurring around the 45th day. Meanwhile, S-Flw-Apt events are consistently occurring on consecutive days (238502, 238507, and 238510).\n",
      "\n",
      "**Updated Analysis:** A-Dns-Clc1 events predominantly occur on days ranging from 259916 to 260275, with a focus on the 45th day. S-Flw-Apt events continue to consistently occur on consecutive days.\n",
      "\n",
      "**5. Overall Patterns:**\n",
      "\n",
      "The new data reinforces the existing patterns of DNS-related events (A-Dns-Clc1) being followed by false_positive labels and subsequently by another A-Dns-Clc1 event. Additionally, the consistent S-Flw-Apt events suggest a high degree of sophistication in the attacker's tactics.\n",
      "\n",
      "**Updated Analysis:** The existing patterns are reinforced by the new data, with an added layer of complexity introduced by the increasingly consistent S-Flw-Apt events.\n",
      "\n",
      "Here is the **final updated summary:**\n",
      "\n",
      "**1. Event Sequence Dependency:**\n",
      "The sequence of DNS-related events (A-Dns-Clc1) suggests initial reconnaissance followed by malicious activity, with an added pattern of repeated A-Dns-Clc1 events and increasingly consistent S-Flw-Apt events.\n",
      "\n",
      "**2. Event Types:**\n",
      "A-Dns-Clc1 events predominantly occur on the 45th day (with a count of 55), while S-Flw-Apt events consistently occur with counts of 75 on multiple consecutive days.\n",
      "\n",
      "**3. Event Counts:**\n",
      "A-Dns-Clc1 events have varying counts (ranging from 34 to 57), while S-Flw-Apt events continue to consistently occur with a count of 75.\n",
      "\n",
      "**4. Event Days:**\n",
      "A-Dns-Clc1 events predominantly occur on days ranging from 259916 to 260275, with a focus on the 45th day. S-Flw-Apt events consistently occur on consecutive days.\n",
      "\n",
      "**5. Overall Patterns:**\n",
      "The existing patterns are reinforced by the new data, with an added layer of complexity introduced by the increasingly consistent S-Flw-Apt events.\n"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "batch_size= 5\n",
    "batches = [X_train_sort_IP[i:i+N] for i in range(0,batch_size)]\n",
    "\n",
    "rules =''\n",
    "for i in range(batch_size):\n",
    "    print(f\"Reading the {i}th batch data\")\n",
    "    if i==0:\n",
    "        context = {\"sum_ip\": batches[i]}\n",
    "        prompt_rule_formated = prompt_rule.format(**context)\n",
    "        rules = llm(prompt_rule_formated)\n",
    "    else:\n",
    "        context = {\"response\": rules, \"sum_ip\": batches[i]}\n",
    "        prompt_rule_formated = prompt_rule_update.format(**context)\n",
    "        rules = llm(prompt_rule_formated)\n",
    "        \n",
    "    # if i==5:\n",
    "    #     break\n",
    "        \n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prediction task\n",
    "prompt_predict = \"\"\"\n",
    "**# Prediction Task**\n",
    "\n",
    "You are a cybersecurity expert. Each network security event is a sample with a timestamp, event description, IP address, hostname, and class label. \n",
    "Your task is to predict the class label of a sample based on its event description and the 10 preceding events.\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "- Example 1:  \n",
    "  Preceding events: {example_1}  \n",
    "  Current event: {event_2}  \n",
    "  Predicted Class label of current event: {label_1}\n",
    "  \n",
    "- Example 2:  \n",
    "  Preceding events: {example_2}  \n",
    "  Current event: {event_2}  \n",
    "  Predicted Class label of current event: {label_2}\n",
    "  \n",
    "- Example 3:  \n",
    "  Preceding events: {example_3}  \n",
    "  Current event: {event_3}  \n",
    "  Predicted Class label of current event: {label_3}\n",
    "  \n",
    "- Example 4:  \n",
    "  Preceding events: {example_4}  \n",
    "  Current event: {event_4}  \n",
    "  Predicted Class label of current event: {label_4}\n",
    "\n",
    "\n",
    "\n",
    "**Task:**  \n",
    "Given the the preceding events, current event, and previous learned {rules}, predict the class label directly without algorithm: \n",
    "Preceding events: {task_precedings}  \n",
    "Current event: {task_event}\n",
    "\n",
    "Format your prediction and analysis as a numbered list:\n",
    " 1. Prediction result: [predicted class label]\n",
    " 2. The reason you make this prediction: \n",
    "      (1) Preceding events analysis: [ analysis the preceding events]\n",
    "      (2) Current event analysis:    [ analysis the current event]\n",
    " The  predicted class label are one of the following 11 types: 'cracking','dirb','dnsteal','false_positive','network_scans',\\\n",
    "  'privilege_escalation','reverse_shell','service_scans','service_stop','webshell','wpscan'. \n",
    "\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = var4prompt(examples_record,n=3)\n",
    "context[f\"example_{4}\"] = predict_withcontext.iloc[0:10,0:5]\n",
    "context[f\"event_{4}\"]   = predict_withcontext.iloc[10][0:5]\n",
    "context[f\"label_{4}\"]   = predict_withcontext.iloc[10][5]\n",
    "\n",
    "context[f\"task_precedings\"] = predict_withcontext.iloc[1:11,0:5]\n",
    "context[f\"task_event\"]      = predict_withcontext.iloc[11,0:5]\n",
    "context['rules'] = rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "formated_prompt_predict = prompt_predict.format(**context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the analysis provided, I predict the class label as:\n",
      "\n",
      "**1. Prediction result: [false_positive]**\n",
      "\n",
      "The reason I make this prediction is:\n",
      "\n",
      "**2. The reason you make this prediction:**\n",
      "\n",
      "**(1) Preceding events analysis:** \n",
      "The preceding events all have the same type (\"W-Sys-Dov\") which suggests a repetitive and likely legitimate activity, such as Dovecot authentication success. There are no indicators of malicious activity in the preceding events.\n",
      "\n",
      "**(2) Current event analysis:**\n",
      "The current event is also a \"Wazuh: Dovecot Authentication Success\" with the host \"mail\". This further reinforces the likelihood that the current event is also a false positive, given the repetitive and legitimate nature of the preceding events.\n",
      "\n",
      "Note that there are no indicators of malicious activity in either the preceding or current events. The repetitive pattern of legitimate events suggests that the current event is likely a false positive.\n"
     ]
    }
   ],
   "source": [
    "response = llm(formated_prompt_predict)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Prediction result: **A-All-Evt**\n",
    "2. The reason you make this prediction:\n",
    "   (1) Preceding events analysis: \n",
    "      - The preceding events are all related to web server 400 error codes, with multiple occurrences on consecutive days.\n",
    "      - Events A-Dns-Clc1 have been consistently occurring on the 45th day, suggesting a pattern of DNS-related reconnaissance.\n",
    "      - The presence of false_positive labels and repeated S-Flw-Apt events indicates a sophisticated attacker's tactics.\n",
    "   (2) Current event analysis: \n",
    "      - The current event is also related to web server 400 error codes, with no indication of malicious activity.\n",
    "      - Considering the preceding events' patterns and the lack of malicious indicators in the current event, it's likely that this event is another occurrence of A-All-Evt (all events)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorchcuda",
   "language": "python",
   "name": "mytorchcuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5661b7e36a53110a99ebc4525200c7d03909fe5487ce7ebd9c6e1792b76c4f8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
